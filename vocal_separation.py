# -*- coding: utf-8 -*-
"""vocal_separation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iUOaohwOCPI6jreKi4ANtO3fLGqXUVmg
"""

import numpy as np
import scipy
from scipy import signal
from scipy.io import wavfile
import matplotlib.pyplot as plt
import pickle
import random

file_path = '../dsp_lab/lab_4/0.wav'

filter_time_width = 8
filter_frequency_width = 8

win_length = 2048
hop_lenght = 512

instruments_gain = 3 # >= 1
vocal_gain = 2 # >= 1

instruments_margin = 1 # [0,...,1]
vocal_margin = 1 # [0,...,1]


def amp_to_db(x):
    return 20 * np.log10(np.maximum(1e-5, x))


def db_to_amp(x):
    return np.power(10.0, x * 0.05)


def plot_spectogram(t, f, stft, title=''):
    fig, ax = plt.subplots(figsize=(14, 5))
    ax.set_title(title)
    ax.set_xlabel("Time")
    ax.set_ylabel("Frequency")
    ax.pcolormesh(t, f, stft, vmin=-1 * np.max(np.abs(stft)), vmax=np.max(np.abs(stft)))
    plt.show()

sample_rate, music = wavfile.read(file_path)
# Выделяем участок с инструментами
instruments = music[int(8*sample_rate):int(10*sample_rate)]

# Краткосрочные преобразования Фурье
m_f, m_t, music_stft = signal.stft(music, fs=sample_rate, noverlap=hop_lenght, nperseg=win_length)
# Преобразование в логарифимическую шкалу
music_stft_db = amp_to_db(np.abs(music_stft))
# Минимальный уровень сигнала
music_min_gain_db = np.min(music_stft_db)

plot_spectogram(m_t, m_f, music_stft_db, 'Orig')

# Краткосрочные преобразования Фурье для участка с интрументами
i_f, i_t, instruments_stft = signal.stft(instruments, fs=sample_rate, noverlap=hop_lenght, nperseg=win_length)
# В Децибеллы 
instruments_stft_db = amp_to_db(np.abs(instruments_stft))
# Подсчет среднего и отклонения для каждой из частот
mean_instruments_stft = np.mean(instruments_stft_db, axis=1)
std_instruments_stft = np.std(instruments_stft_db, axis=1)
# Определение порога мощности по частотам 
instruments_threshold = mean_instruments_stft + std_instruments_stft * 3

print(instruments_threshold.shape, music_stft_db.shape)
# Расширение по всей временнной области
threshold_db = np.repeat(np.expand_dims(instruments_threshold, axis=0), np.shape(music_stft)[1], axis=0).T


#plot_spectogram(m_t, m_f, threshold_db, 'Threshold')

# Сглаживающий фильтр
smoothing_filter = np.outer(signal.cosine(8), signal.cosine(8))
smoothing_filter = smoothing_filter / np.sum(smoothing_filter)

#plt.imshow(smoothing_filter)
#plt.title('Filter kernel')
#plt.show()

# Определение маски инструментов
instruments_mask_db = music_stft_db < threshold_db
#plot_spectogram(m_t, m_f, instruments_mask_db, 'Intruments mask')

# Сглаживаие
instruments_mask_db = signal.fftconvolve(instruments_mask_db, smoothing_filter, mode="same")
instruments_mask_db = instruments_mask_db * instruments_margin
#plot_spectogram(m_t, m_f, instruments_mask_db, 'Filtered intruments mask')

# Отделение маски
vocal_stft_db = (music_stft_db * (1-instruments_mask_db)) + music_min_gain_db * instruments_mask_db
#plot_spectogram(m_t, m_f, vocal_stft_db, 'Vocal')


# Сохранение мнимой части
vocal_imag = np.imag(music_stft) * (1 - instruments_mask_db)
vocal_stft = db_to_amp(vocal_stft_db) + (1j * vocal_imag)

_, vocal = signal.istft(vocal_stft, fs=sample_rate, noverlap=hop_lenght, nperseg=win_length)
wavfile.write('vocal.wav', rate=sample_rate, data=vocal.astype(np.int16))




# Отделение маски
vocal_part = music[int(0*sample_rate):int(2*sample_rate)]
v_f, v_t, vocal_stft = signal.stft(vocal_part, fs=sample_rate, noverlap=hop_lenght, nperseg=win_length)
vocal_stft_db = amp_to_db(np.abs(vocal_stft))
# Подсчет среднего и отклонения для каждой из частот
mean_vocal_stft = np.mean(vocal_stft_db, axis=1)
std_vocal_stft = np.std(vocal_stft_db, axis=1)
# Определение порога мощности по частотам
vocal_threshold = mean_vocal_stft + std_vocal_stft * 3

vocal_pattern = vocal_threshold - instruments_threshold

plt.figure(figsize=(14, 5))
plt.plot(instruments_threshold)
plt.plot(vocal_threshold)
plt.plot(vocal_pattern)
plt.title('Threshold')
plt.xlabel('Frequency')
plt.ylabel('Level')
plt.show()

vocal_pattern = np.repeat(np.expand_dims(vocal_pattern, axis=0), np.shape(music_stft)[1], axis=0).T

instruments_mask_db = 1 - instruments_mask_db
background_stft_db = ((music_stft_db - vocal_pattern) * (1-instruments_mask_db)) + music_min_gain_db * instruments_mask_db
plot_spectogram(m_t, m_f, background_stft_db, 'background')


# Сохранение мнимой части
background_imag = np.imag(music_stft) * (1 - instruments_mask_db)
background_stft = db_to_amp(background_stft_db) + (1j * background_imag)

_, vocal = signal.istft(background_stft, fs=sample_rate, noverlap=hop_lenght, nperseg=win_length)
wavfile.write('background.wav', rate=sample_rate, data=vocal.astype(np.int16))